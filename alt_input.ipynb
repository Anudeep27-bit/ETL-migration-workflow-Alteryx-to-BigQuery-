{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM64lrVmwL4Z6HBo6oyz86i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anudeep27-bit/ETL-migration-workflow-Alteryx-to-BigQuery-/blob/main/alt_input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DOX8H0kYBai",
        "outputId": "d0e789de-cdc4-4dc3-f5b2-4c47f628c17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.11/dist-packages (3.25.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.1.4)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.11/dist-packages (4.1.3)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.27.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.7.2)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (24.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.1)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (0.4.1)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.25.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.69.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.62.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.5.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2>=0.9.1->oauth2client) (3.2.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2024.12.14)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-bigquery pandas gspread oauth2client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Google Drive folder ID (replace with your folder's ID)\n",
        "drive_folder_id = \"1nZPRsIy8_u5tJ7vNpSLEtg6c0qy6nMvG\"\n",
        "\n",
        "# Authenticate Google Drive API\n",
        "def authenticate_drive():\n",
        "    from google.oauth2 import service_account\n",
        "    SCOPES = ['https://www.googleapis.com/auth/drive']\n",
        "    SERVICE_ACCOUNT_FILE = 'path_to_your_service_account_key.json'  # Replace with your service account key file\n",
        "    credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
        "    return build('drive', 'v3', credentials=credentials)\n",
        "\n",
        "drive_service = authenticate_drive()\n",
        "\n",
        "# Define the query\n",
        "query_template = \"\"\"\n",
        "    SELECT\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, SUM(quantity) AS quantity,\n",
        "        number_of_months, SUM(amount) AS amount, rate_USD, amount_USD, bill_code_sfdc, unit_of_measure,\n",
        "        storage_from_date, storage_to_date, original_invoice_id, revenue_recognition_date,\n",
        "        bill_in_advance_flag, bill_in_advance_month_number, revenue_recognition_amount,\n",
        "        allocated_quantity\n",
        "    FROM irm-fin-acct-dp-prod.rpt_billing.billing_detail\n",
        "    WHERE\n",
        "        transaction_region = \"NA\"\n",
        "        AND calendar_date BETWEEN \"{start_date}\" AND \"{end_date}\"\n",
        "        AND revenue_classification = \"Storage\"\n",
        "        AND product_summary_type = \"Records Management\"\n",
        "    GROUP BY ALL\n",
        "\"\"\"\n",
        "\n",
        "# Define the months for extraction\n",
        "months = pd.date_range(start=\"2025-01-01\", end=\"2025-01-31\", freq=\"MS\")\n",
        "\n",
        "for start_date in months:\n",
        "    start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
        "    end_date_str = (start_date + pd.offsets.MonthEnd(0)).strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    # Format the query for the specific month\n",
        "    query = query_template.format(start_date=start_date_str, end_date=end_date_str)\n",
        "\n",
        "    # Run the query\n",
        "    query_job = client.query(query)\n",
        "    df = query_job.to_dataframe()\n",
        "\n",
        "    # Save to local CSV\n",
        "    file_name = f\"data_{start_date.strftime('%Y_%m')}.csv\"\n",
        "    df.to_csv(file_name, index=False)\n",
        "\n",
        "    # Upload to Google Drive\n",
        "    file_metadata = {'name': file_name, 'parents': [drive_folder_id]}\n",
        "    media = MediaFileUpload(file_name, mimetype='text/csv')\n",
        "    drive_service.files().create(body=file_metadata, media_body=media).execute()\n",
        "\n",
        "    # Delete the local file\n",
        "    os.remove(file_name)\n",
        "\n",
        "print(\"All files successfully uploaded to Google Drive!\")\n"
      ],
      "metadata": {
        "id": "G9H1ppVzYP_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate with Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Authenticate with Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HQN5Cb1eFZX",
        "outputId": "fc4eb4d0-f439-4a57-8986-6dc5485e72ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gspread pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVToVVLWee7P",
        "outputId": "db725826-d00c-47a0-c963-4e433c7facd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (6.1.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.2)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (2024.12.14)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = bigquery.Client(project=\"irm-fin-acct-dp-prod\")\n"
      ],
      "metadata": {
        "id": "HzLZaJJPFVRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List datasets in the project\n",
        "datasets = list(client.list_datasets())\n",
        "print(\"Datasets in the project:\")\n",
        "for dataset in datasets:\n",
        "    print(f\"- {dataset.dataset_id}\")\n",
        "\n",
        "# List tables in the 'rpt_billing' dataset\n",
        "tables = list(client.list_tables(\"rpt_billing\"))  # Replace with the dataset name\n",
        "print(\"\\nTables in the 'rpt_billing' dataset:\")\n",
        "for table in tables:\n",
        "    print(f\"- {table.table_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ij1AI70iFc6p",
        "outputId": "48abc556-26bc-4d7a-d723-694995e6b48e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets in the project:\n",
            "- API_Analysis\n",
            "- CTesting\n",
            "- Etl_alt_BQ\n",
            "- IYR\n",
            "- IYR_Process\n",
            "- IYR_Process_2025\n",
            "- Insight_Scanning\n",
            "- Legacy_EDP_Sandbox\n",
            "- Matterhorn\n",
            "- NonEDPDatasets\n",
            "- OS_REPORTING\n",
            "- RCOE_Requests\n",
            "- RO_Tracker\n",
            "- aggregated_one_stream_static\n",
            "- global_customer_revenue\n",
            "- inventory\n",
            "- inventory_nonprod\n",
            "- onestream\n",
            "- rpt_billing\n",
            "- salesforce_core\n",
            "\n",
            "Tables in the 'rpt_billing' dataset:\n",
            "- billing_detail\n",
            "- orders_not_billed\n",
            "- rpt_ar_aggregated\n",
            "- rpt_monthly_customer_invoice_billing_summary\n",
            "- rpt_skp_order_service_fee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List all tables in the 'rpt_billing' dataset\n",
        "dataset_id = \"rpt_billing\"\n",
        "project_id = \"irm-fin-acct-dp-prod\"\n",
        "\n",
        "tables = list(client.list_tables(dataset_id))\n",
        "print(f\"Tables in dataset '{dataset_id}':\")\n",
        "for table in tables:\n",
        "    print(f\"- {table.table_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ae3TVzMG0rL",
        "outputId": "e8848580-5f2f-48a2-c07e-066c0499039b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tables in dataset 'rpt_billing':\n",
            "- billing_detail\n",
            "- orders_not_billed\n",
            "- rpt_ar_aggregated\n",
            "- rpt_monthly_customer_invoice_billing_summary\n",
            "- rpt_skp_order_service_fee\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "SELECT *\n",
        "FROM `irm-fin-acct-dp-prod.rpt_billing.billing_detail`\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query and load the results into a DataFrame\n",
        "query_job = client.query(query)\n",
        "df = query_job.to_dataframe()\n",
        "\n",
        "# Display the first few rows\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjz7kyttG44v",
        "outputId": "6b08a720-5bac-4491-a681-94ebb34c8a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  billing_system invoicing_country_code calendar_date invoice_date  \\\n",
            "0            SKP                    USA    2020-08-13   2020-01-31   \n",
            "1            SKP                    USA    2020-08-13   2020-01-31   \n",
            "2            SKP                    USA    2020-08-31   2020-08-31   \n",
            "3            SKP                    USA    2020-08-31   2020-08-31   \n",
            "4            SKP                    USA    2020-08-12   2020-03-31   \n",
            "\n",
            "  adjustment_date year_month  year_month_num  year transaction_org  \\\n",
            "0      2020-08-13    2020-08          202008  2020             GRO   \n",
            "1      2020-08-13    2020-08          202008  2020             GRO   \n",
            "2             NaT    2020-08          202008  2020             GRO   \n",
            "3             NaT    2020-08          202008  2020             GRO   \n",
            "4      2020-08-12    2020-08          202008  2020             GRO   \n",
            "\n",
            "  transaction_country_code  ... trip_id remit_level revenue_recognition_date  \\\n",
            "0                      USA  ...       0          1M               2020-01-31   \n",
            "1                      USA  ...       0          1M               2020-01-31   \n",
            "2                      USA  ...       0          1S               2020-08-31   \n",
            "3                      USA  ...       0          1S               2020-08-31   \n",
            "4                      USA  ...       0          1S               2020-03-31   \n",
            "\n",
            "  bill_in_advance_flag bill_in_advance_month_number  \\\n",
            "0                False                            0   \n",
            "1                False                            0   \n",
            "2                False                            0   \n",
            "3                False                            0   \n",
            "4                False                            0   \n",
            "\n",
            "  revenue_recognition_amount revenue_recognition_usd_amount  \\\n",
            "0            -1349.200000000                -1349.200000000   \n",
            "1              -18.610000000                  -18.610000000   \n",
            "2             1777.360000000                 1777.360000000   \n",
            "3             3000.000000000                 3000.000000000   \n",
            "4            -2401.160000000                -2401.160000000   \n",
            "\n",
            "  allocated_quantity billing_system_derived_gcr               updated_date  \n",
            "0       -1.000000000                       None 2025-01-23 12:51:21.895261  \n",
            "1       -1.000000000                       None 2025-01-23 12:51:21.895261  \n",
            "2        1.000000000                       None 2025-01-23 12:51:21.895261  \n",
            "3        1.000000000                       None 2025-01-23 12:51:21.895261  \n",
            "4   -44466.000000000                       None 2025-01-23 12:51:21.895261  \n",
            "\n",
            "[5 rows x 93 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the folder in Google Drive\n",
        "export_folder = \"/content/drive/My Drive/BigQuery_Exports\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(export_folder):\n",
        "    os.makedirs(export_folder)\n",
        "    print(f\"Folder created: {export_folder}\")\n",
        "else:\n",
        "    print(f\"Folder already exists: {export_folder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUtskjj8IU1-",
        "outputId": "ddb4016c-19d5-4cd4-b887-ac38e1db6846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Folder created: /content/drive/My Drive/BigQuery_Exports\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate with BigQuery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "client = bigquery.Client(project=\"irm-fin-acct-dp-prod\")  # Replace with your project ID\n",
        "\n",
        "# Define the export folder\n",
        "export_folder = \"/content/drive/My Drive/BQ_EXP/BigQuery_Exports_NA\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(export_folder):\n",
        "    os.makedirs(export_folder)\n",
        "    print(f\"Folder created: {export_folder}\")\n",
        "else:\n",
        "    print(f\"Folder already exists: {export_folder}\")\n",
        "\n",
        "# Define the date ranges\n",
        "months = pd.date_range(start=\"2023-01-01\", end=\"2024-12-31\", freq=\"MS\")  # Monthly start dates\n",
        "\n",
        "# Loop through each month and query data\n",
        "for start_date in months:\n",
        "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "    end_date_str = (start_date + pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')  # End of the month\n",
        "\n",
        "    # Query for the current month\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, SUM(quantity) AS quantity,\n",
        "        number_of_months, SUM(amount) AS amount, rate_USD, amount_USD, bill_code_sfdc, unit_of_measure,\n",
        "        storage_from_date, storage_to_date, original_invoice_id, revenue_recognition_date,\n",
        "        bill_in_advance_flag, bill_in_advance_month_number, revenue_recognition_amount,\n",
        "        allocated_quantity\n",
        "    FROM `irm-fin-acct-dp-prod.rpt_billing.billing_detail`\n",
        "    WHERE\n",
        "        transaction_region = 'NA'\n",
        "        AND calendar_date BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
        "        AND revenue_classification = 'Storage'\n",
        "        AND product_summary_type = 'Records Management'\n",
        "    GROUP BY\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, number_of_months, rate_USD,\n",
        "        amount_USD, bill_code_sfdc, unit_of_measure, storage_from_date, storage_to_date,\n",
        "        original_invoice_id, revenue_recognition_date, bill_in_advance_flag,\n",
        "        bill_in_advance_month_number, revenue_recognition_amount, allocated_quantity\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and fetch the results\n",
        "    query_job = client.query(query)\n",
        "    df = query_job.to_dataframe()  # Load query results into a Pandas DataFrame\n",
        "\n",
        "    # Save the results to a CSV file in Google Drive\n",
        "    file_name = f\"BigQuery_Data_{start_date.strftime('%Y_%m')}.csv\"\n",
        "    output_path = os.path.join(export_folder, file_name)\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Data for {start_date_str} to {end_date_str} saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wnZQ8qSIbMJ",
        "outputId": "36f4f385-cc8a-418b-89a3-edfe5c4c83aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Folder already exists: /content/drive/My Drive/BigQuery_Exports\n",
            "Data for 2023-01-01 to 2023-01-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_01.csv\n",
            "Data for 2023-02-01 to 2023-02-28 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_02.csv\n",
            "Data for 2023-03-01 to 2023-03-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_03.csv\n",
            "Data for 2023-04-01 to 2023-04-30 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_04.csv\n",
            "Data for 2023-05-01 to 2023-05-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_05.csv\n",
            "Data for 2023-06-01 to 2023-06-30 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_06.csv\n",
            "Data for 2023-07-01 to 2023-07-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_07.csv\n",
            "Data for 2023-08-01 to 2023-08-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_08.csv\n",
            "Data for 2023-09-01 to 2023-09-30 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_09.csv\n",
            "Data for 2023-10-01 to 2023-10-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_10.csv\n",
            "Data for 2023-11-01 to 2023-11-30 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_11.csv\n",
            "Data for 2023-12-01 to 2023-12-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2023_12.csv\n",
            "Data for 2024-01-01 to 2024-01-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_01.csv\n",
            "Data for 2024-02-01 to 2024-02-29 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_02.csv\n",
            "Data for 2024-03-01 to 2024-03-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_03.csv\n",
            "Data for 2024-04-01 to 2024-04-30 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_04.csv\n",
            "Data for 2024-05-01 to 2024-05-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_05.csv\n",
            "Data for 2024-06-01 to 2024-06-30 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_06.csv\n",
            "Data for 2024-07-01 to 2024-07-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_07.csv\n",
            "Data for 2024-08-01 to 2024-08-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_08.csv\n",
            "Data for 2024-09-01 to 2024-09-30 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_09.csv\n",
            "Data for 2024-10-01 to 2024-10-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_10.csv\n",
            "Data for 2024-11-01 to 2024-11-30 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_11.csv\n",
            "Data for 2024-12-01 to 2024-12-31 saved to /content/drive/My Drive/BigQuery_Exports/BigQuery_Data_2024_12.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate with BigQuery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "client = bigquery.Client(project=\"irm-fin-acct-dp-prod\")  # Replace with your project ID\n",
        "\n",
        "# Define the export folder\n",
        "export_folder = \"/content/drive/My Drive/BQ_EXP/BigQuery_Exports_India\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(export_folder):\n",
        "    os.makedirs(export_folder)\n",
        "    print(f\"Folder created: {export_folder}\")\n",
        "else:\n",
        "    print(f\"Folder already exists: {export_folder}\")\n",
        "\n",
        "# Define the date ranges\n",
        "months = pd.date_range(start=\"2023-01-01\", end=\"2024-12-31\", freq=\"MS\")  # Monthly start dates\n",
        "\n",
        "# Loop through each month and query data\n",
        "for start_date in months:\n",
        "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "    end_date_str = (start_date + pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')  # End of the month\n",
        "\n",
        "    # Query for the current month\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, SUM(quantity) AS quantity,\n",
        "        number_of_months, SUM(amount) AS amount, rate_USD, amount_USD, bill_code_sfdc, unit_of_measure,\n",
        "        storage_from_date, storage_to_date, original_invoice_id, revenue_recognition_date,\n",
        "        bill_in_advance_flag, bill_in_advance_month_number, revenue_recognition_amount,\n",
        "        allocated_quantity\n",
        "    FROM `irm-fin-acct-dp-prod.rpt_billing.billing_detail`\n",
        "    WHERE\n",
        "        transaction_region = 'India'\n",
        "        AND calendar_date BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
        "        AND revenue_classification = 'Storage'\n",
        "        AND product_summary_type = 'Records Management'\n",
        "    GROUP BY\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, number_of_months, rate_USD,\n",
        "        amount_USD, bill_code_sfdc, unit_of_measure, storage_from_date, storage_to_date,\n",
        "        original_invoice_id, revenue_recognition_date, bill_in_advance_flag,\n",
        "        bill_in_advance_month_number, revenue_recognition_amount, allocated_quantity\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and fetch the results\n",
        "    query_job = client.query(query)\n",
        "    df = query_job.to_dataframe()  # Load query results into a Pandas DataFrame\n",
        "\n",
        "    # Save the results to a CSV file in Google Drive\n",
        "    file_name = f\"BigQuery_Data_{start_date.strftime('%Y_%m')}.csv\"\n",
        "    output_path = os.path.join(export_folder, file_name)\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Data for {start_date_str} to {end_date_str} saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3vZZSRKjr4k",
        "outputId": "9199d490-9faa-4eea-ad4d-1a11b3f049f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Folder created: /content/drive/My Drive/BigQuery_Exports_India\n",
            "Data for 2023-01-01 to 2023-01-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_01.csv\n",
            "Data for 2023-02-01 to 2023-02-28 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_02.csv\n",
            "Data for 2023-03-01 to 2023-03-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_03.csv\n",
            "Data for 2023-04-01 to 2023-04-30 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_04.csv\n",
            "Data for 2023-05-01 to 2023-05-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_05.csv\n",
            "Data for 2023-06-01 to 2023-06-30 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_06.csv\n",
            "Data for 2023-07-01 to 2023-07-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_07.csv\n",
            "Data for 2023-08-01 to 2023-08-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_08.csv\n",
            "Data for 2023-09-01 to 2023-09-30 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_09.csv\n",
            "Data for 2023-10-01 to 2023-10-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_10.csv\n",
            "Data for 2023-11-01 to 2023-11-30 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_11.csv\n",
            "Data for 2023-12-01 to 2023-12-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2023_12.csv\n",
            "Data for 2024-01-01 to 2024-01-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_01.csv\n",
            "Data for 2024-02-01 to 2024-02-29 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_02.csv\n",
            "Data for 2024-03-01 to 2024-03-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_03.csv\n",
            "Data for 2024-04-01 to 2024-04-30 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_04.csv\n",
            "Data for 2024-05-01 to 2024-05-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_05.csv\n",
            "Data for 2024-06-01 to 2024-06-30 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_06.csv\n",
            "Data for 2024-07-01 to 2024-07-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_07.csv\n",
            "Data for 2024-08-01 to 2024-08-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_08.csv\n",
            "Data for 2024-09-01 to 2024-09-30 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_09.csv\n",
            "Data for 2024-10-01 to 2024-10-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_10.csv\n",
            "Data for 2024-11-01 to 2024-11-30 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_11.csv\n",
            "Data for 2024-12-01 to 2024-12-31 saved to /content/drive/My Drive/BigQuery_Exports_India/BigQuery_Data_2024_12.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate with BigQuery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "client = bigquery.Client(project=\"irm-fin-acct-dp-prod\")  # Replace with your project ID\n",
        "\n",
        "# Define the export folder\n",
        "export_folder = \"/content/drive/My Drive/BQ_EXP/BigQuery_Exports_LATAM\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(export_folder):\n",
        "    os.makedirs(export_folder)\n",
        "    print(f\"Folder created: {export_folder}\")\n",
        "else:\n",
        "    print(f\"Folder already exists: {export_folder}\")\n",
        "\n",
        "# Define the date ranges\n",
        "months = pd.date_range(start=\"2023-01-01\", end=\"2024-12-31\", freq=\"MS\")  # Monthly start dates\n",
        "\n",
        "# Loop through each month and query data\n",
        "for start_date in months:\n",
        "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "    end_date_str = (start_date + pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')  # End of the month\n",
        "\n",
        "    # Query for the current month\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, SUM(quantity) AS quantity,\n",
        "        number_of_months, SUM(amount) AS amount, rate_USD, amount_USD, bill_code_sfdc, unit_of_measure,\n",
        "        storage_from_date, storage_to_date, original_invoice_id, revenue_recognition_date,\n",
        "        bill_in_advance_flag, bill_in_advance_month_number, revenue_recognition_amount,\n",
        "        allocated_quantity\n",
        "    FROM `irm-fin-acct-dp-prod.rpt_billing.billing_detail`\n",
        "    WHERE\n",
        "        transaction_region = 'LATAM'\n",
        "        AND calendar_date BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
        "        AND revenue_classification = 'Storage'\n",
        "        AND product_summary_type = 'Records Management'\n",
        "    GROUP BY\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, number_of_months, rate_USD,\n",
        "        amount_USD, bill_code_sfdc, unit_of_measure, storage_from_date, storage_to_date,\n",
        "        original_invoice_id, revenue_recognition_date, bill_in_advance_flag,\n",
        "        bill_in_advance_month_number, revenue_recognition_amount, allocated_quantity\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and fetch the results\n",
        "    query_job = client.query(query)\n",
        "    df = query_job.to_dataframe()  # Load query results into a Pandas DataFrame\n",
        "\n",
        "    # Save the results to a CSV file in Google Drive\n",
        "    file_name = f\"BigQuery_Data_{start_date.strftime('%Y_%m')}.csv\"\n",
        "    output_path = os.path.join(export_folder, file_name)\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Data for {start_date_str} to {end_date_str} saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeNMu5DNlIjY",
        "outputId": "fc901816-8a90-429f-b499-e0e2987ff30a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Folder created: /content/drive/My Drive/BigQuery_Exports_LATAM\n",
            "Data for 2023-01-01 to 2023-01-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_01.csv\n",
            "Data for 2023-02-01 to 2023-02-28 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_02.csv\n",
            "Data for 2023-03-01 to 2023-03-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_03.csv\n",
            "Data for 2023-04-01 to 2023-04-30 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_04.csv\n",
            "Data for 2023-05-01 to 2023-05-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_05.csv\n",
            "Data for 2023-06-01 to 2023-06-30 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_06.csv\n",
            "Data for 2023-07-01 to 2023-07-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_07.csv\n",
            "Data for 2023-08-01 to 2023-08-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_08.csv\n",
            "Data for 2023-09-01 to 2023-09-30 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_09.csv\n",
            "Data for 2023-10-01 to 2023-10-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_10.csv\n",
            "Data for 2023-11-01 to 2023-11-30 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_11.csv\n",
            "Data for 2023-12-01 to 2023-12-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2023_12.csv\n",
            "Data for 2024-01-01 to 2024-01-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_01.csv\n",
            "Data for 2024-02-01 to 2024-02-29 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_02.csv\n",
            "Data for 2024-03-01 to 2024-03-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_03.csv\n",
            "Data for 2024-04-01 to 2024-04-30 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_04.csv\n",
            "Data for 2024-05-01 to 2024-05-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_05.csv\n",
            "Data for 2024-06-01 to 2024-06-30 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_06.csv\n",
            "Data for 2024-07-01 to 2024-07-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_07.csv\n",
            "Data for 2024-08-01 to 2024-08-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_08.csv\n",
            "Data for 2024-09-01 to 2024-09-30 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_09.csv\n",
            "Data for 2024-10-01 to 2024-10-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_10.csv\n",
            "Data for 2024-11-01 to 2024-11-30 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_11.csv\n",
            "Data for 2024-12-01 to 2024-12-31 saved to /content/drive/My Drive/BigQuery_Exports_LATAM/BigQuery_Data_2024_12.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate with BigQuery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "client = bigquery.Client(project=\"irm-fin-acct-dp-prod\")  # Replace with your project ID\n",
        "\n",
        "# Define the export folder\n",
        "export_folder = \"/content/drive/My Drive/BQ_EXP/BigQuery_Exports_EMEA\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(export_folder):\n",
        "    os.makedirs(export_folder)\n",
        "    print(f\"Folder created: {export_folder}\")\n",
        "else:\n",
        "    print(f\"Folder already exists: {export_folder}\")\n",
        "\n",
        "# Define the date ranges\n",
        "months = pd.date_range(start=\"2023-01-01\", end=\"2024-12-31\", freq=\"MS\")  # Monthly start dates\n",
        "\n",
        "# Loop through each month and query data\n",
        "for start_date in months:\n",
        "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "    end_date_str = (start_date + pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')  # End of the month\n",
        "\n",
        "    # Query for the current month\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, SUM(quantity) AS quantity,\n",
        "        number_of_months, SUM(amount) AS amount, rate_USD, amount_USD, bill_code_sfdc, unit_of_measure,\n",
        "        storage_from_date, storage_to_date, original_invoice_id, revenue_recognition_date,\n",
        "        bill_in_advance_flag, bill_in_advance_month_number, revenue_recognition_amount,\n",
        "        allocated_quantity\n",
        "    FROM `irm-fin-acct-dp-prod.rpt_billing.billing_detail`\n",
        "    WHERE\n",
        "        transaction_region = 'EMEA'\n",
        "        AND calendar_date BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
        "        AND revenue_classification = 'Storage'\n",
        "        AND product_summary_type = 'Records Management'\n",
        "    GROUP BY\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, number_of_months, rate_USD,\n",
        "        amount_USD, bill_code_sfdc, unit_of_measure, storage_from_date, storage_to_date,\n",
        "        original_invoice_id, revenue_recognition_date, bill_in_advance_flag,\n",
        "        bill_in_advance_month_number, revenue_recognition_amount, allocated_quantity\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and fetch the results\n",
        "    query_job = client.query(query)\n",
        "    df = query_job.to_dataframe()  # Load query results into a Pandas DataFrame\n",
        "\n",
        "    # Save the results to a CSV file in Google Drive\n",
        "    file_name = f\"BigQuery_Data_{start_date.strftime('%Y_%m')}.csv\"\n",
        "    output_path = os.path.join(export_folder, file_name)\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Data for {start_date_str} to {end_date_str} saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZtA--avlPJv",
        "outputId": "dc3cc85d-b95a-4f6a-a78e-2cb53049f908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Folder created: /content/drive/My Drive/BigQuery_Exports_EMEA\n",
            "Data for 2023-01-01 to 2023-01-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_01.csv\n",
            "Data for 2023-02-01 to 2023-02-28 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_02.csv\n",
            "Data for 2023-03-01 to 2023-03-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_03.csv\n",
            "Data for 2023-04-01 to 2023-04-30 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_04.csv\n",
            "Data for 2023-05-01 to 2023-05-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_05.csv\n",
            "Data for 2023-06-01 to 2023-06-30 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_06.csv\n",
            "Data for 2023-07-01 to 2023-07-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_07.csv\n",
            "Data for 2023-08-01 to 2023-08-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_08.csv\n",
            "Data for 2023-09-01 to 2023-09-30 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_09.csv\n",
            "Data for 2023-10-01 to 2023-10-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_10.csv\n",
            "Data for 2023-11-01 to 2023-11-30 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_11.csv\n",
            "Data for 2023-12-01 to 2023-12-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2023_12.csv\n",
            "Data for 2024-01-01 to 2024-01-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_01.csv\n",
            "Data for 2024-02-01 to 2024-02-29 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_02.csv\n",
            "Data for 2024-03-01 to 2024-03-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_03.csv\n",
            "Data for 2024-04-01 to 2024-04-30 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_04.csv\n",
            "Data for 2024-05-01 to 2024-05-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_05.csv\n",
            "Data for 2024-06-01 to 2024-06-30 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_06.csv\n",
            "Data for 2024-07-01 to 2024-07-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_07.csv\n",
            "Data for 2024-08-01 to 2024-08-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_08.csv\n",
            "Data for 2024-09-01 to 2024-09-30 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_09.csv\n",
            "Data for 2024-10-01 to 2024-10-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_10.csv\n",
            "Data for 2024-11-01 to 2024-11-30 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_11.csv\n",
            "Data for 2024-12-01 to 2024-12-31 saved to /content/drive/My Drive/BigQuery_Exports_EMEA/BigQuery_Data_2024_12.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import bigquery\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Authenticate with BigQuery\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "client = bigquery.Client(project=\"irm-fin-acct-dp-prod\")  # Replace with your project ID\n",
        "\n",
        "# Define the export folder\n",
        "export_folder = \"/content/drive/My Drive/BQ_EXP/BigQuery_Exports_APAC\"\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "if not os.path.exists(export_folder):\n",
        "    os.makedirs(export_folder)\n",
        "    print(f\"Folder created: {export_folder}\")\n",
        "else:\n",
        "    print(f\"Folder already exists: {export_folder}\")\n",
        "\n",
        "# Define the date ranges\n",
        "months = pd.date_range(start=\"2023-01-01\", end=\"2024-12-31\", freq=\"MS\")  # Monthly start dates\n",
        "\n",
        "# Loop through each month and query data\n",
        "for start_date in months:\n",
        "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
        "    end_date_str = (start_date + pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')  # End of the month\n",
        "\n",
        "    # Query for the current month\n",
        "    query = f\"\"\"\n",
        "    SELECT\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, SUM(quantity) AS quantity,\n",
        "        number_of_months, SUM(amount) AS amount, rate_USD, amount_USD, bill_code_sfdc, unit_of_measure,\n",
        "        storage_from_date, storage_to_date, original_invoice_id, revenue_recognition_date,\n",
        "        bill_in_advance_flag, bill_in_advance_month_number, revenue_recognition_amount,\n",
        "        allocated_quantity\n",
        "    FROM `irm-fin-acct-dp-prod.rpt_billing.billing_detail`\n",
        "    WHERE\n",
        "        transaction_region = 'APAC'\n",
        "        AND calendar_date BETWEEN '{start_date_str}' AND '{end_date_str}'\n",
        "        AND revenue_classification = 'Storage'\n",
        "        AND product_summary_type = 'Records Management'\n",
        "    GROUP BY\n",
        "        billing_system, invoicing_country_code, calendar_date, invoice_date, invoice_id, year_month,\n",
        "        transaction_country_name, transaction_cluster, transaction_region, customer_L1_code,\n",
        "        customer_L1_name, imga, billing_cycle, global_industry_indicator, sf_country_parent_id,\n",
        "        sf_country_parent_name, oracle_account_number, oracle_account_name, salesforce_billing_account,\n",
        "        bill_code, description, product_id, product_summary_type, bill_code_data_source, gl_account,\n",
        "        gl_account_description, revenue_classification, currency, rate, number_of_months, rate_USD,\n",
        "        amount_USD, bill_code_sfdc, unit_of_measure, storage_from_date, storage_to_date,\n",
        "        original_invoice_id, revenue_recognition_date, bill_in_advance_flag,\n",
        "        bill_in_advance_month_number, revenue_recognition_amount, allocated_quantity\n",
        "    \"\"\"\n",
        "\n",
        "    # Execute the query and fetch the results\n",
        "    query_job = client.query(query)\n",
        "    df = query_job.to_dataframe()  # Load query results into a Pandas DataFrame\n",
        "\n",
        "    # Save the results to a CSV file in Google Drive\n",
        "    file_name = f\"BigQuery_Data_{start_date.strftime('%Y_%m')}.csv\"\n",
        "    output_path = os.path.join(export_folder, file_name)\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "    print(f\"Data for {start_date_str} to {end_date_str} saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_J4DAvjlmqp",
        "outputId": "972973a9-669a-486d-df11-9b0ac222ff63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Folder created: /content/drive/My Drive/BigQuery_Exports_APAC\n",
            "Data for 2023-01-01 to 2023-01-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_01.csv\n",
            "Data for 2023-02-01 to 2023-02-28 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_02.csv\n",
            "Data for 2023-03-01 to 2023-03-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_03.csv\n",
            "Data for 2023-04-01 to 2023-04-30 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_04.csv\n",
            "Data for 2023-05-01 to 2023-05-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_05.csv\n",
            "Data for 2023-06-01 to 2023-06-30 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_06.csv\n",
            "Data for 2023-07-01 to 2023-07-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_07.csv\n",
            "Data for 2023-08-01 to 2023-08-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_08.csv\n",
            "Data for 2023-09-01 to 2023-09-30 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_09.csv\n",
            "Data for 2023-10-01 to 2023-10-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_10.csv\n",
            "Data for 2023-11-01 to 2023-11-30 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_11.csv\n",
            "Data for 2023-12-01 to 2023-12-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2023_12.csv\n",
            "Data for 2024-01-01 to 2024-01-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_01.csv\n",
            "Data for 2024-02-01 to 2024-02-29 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_02.csv\n",
            "Data for 2024-03-01 to 2024-03-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_03.csv\n",
            "Data for 2024-04-01 to 2024-04-30 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_04.csv\n",
            "Data for 2024-05-01 to 2024-05-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_05.csv\n",
            "Data for 2024-06-01 to 2024-06-30 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_06.csv\n",
            "Data for 2024-07-01 to 2024-07-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_07.csv\n",
            "Data for 2024-08-01 to 2024-08-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_08.csv\n",
            "Data for 2024-09-01 to 2024-09-30 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_09.csv\n",
            "Data for 2024-10-01 to 2024-10-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_10.csv\n",
            "Data for 2024-11-01 to 2024-11-30 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_11.csv\n",
            "Data for 2024-12-01 to 2024-12-31 saved to /content/drive/My Drive/BigQuery_Exports_APAC/BigQuery_Data_2024_12.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#<<<<<< ##  RUN ALTERYX  ## >>>>**\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6v7hA4DVpi6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Google Colab\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up Google Drive API\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Specify the folder ID\n",
        "folder_id = \"100gMaQKpTdrCx1IYJaAyQgNU75swnVLZ\"  # Your folder ID\n",
        "\n",
        "# Search for CSV files in the folder\n",
        "query = f\"'{folder_id}' in parents and mimeType='text/csv'\"\n",
        "response = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
        "files = response.get('files', [])\n",
        "\n",
        "if not files:\n",
        "    print(\"No CSV files found in the folder.\")\n",
        "else:\n",
        "    print(f\"Found {len(files)} CSV files in the folder:\")\n",
        "    for file in files:\n",
        "        print(f\"Name: {file['name']}, ID: {file['id']}\")\n",
        "\n",
        "# Combine all CSV files into a single DataFrame\n",
        "combined_data = pd.DataFrame()\n",
        "for file in files:\n",
        "    file_name = file['name']\n",
        "    file_id = file['id']\n",
        "\n",
        "    # Download the file content\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    file_path = f\"/content/{file_name}\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(request.execute())\n",
        "\n",
        "    # Load the CSV into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "\n",
        "# Preview the combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Set up BigQuery client\n",
        "project_id = \"irm-fin-acct-dp-prod\"\n",
        "dataset_id = \"Etl_alt_BQ\"\n",
        "table_id = \"PXQ_NA_Clean\"\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Auto-detect schema and load the combined DataFrame into BigQuery\n",
        "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "# Load the combined data to BigQuery\n",
        "job = client.load_table_from_dataframe(combined_data, f\"{project_id}.{dataset_id}.{table_id}\", job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f\"Data successfully loaded into {project_id}.{dataset_id}.{table_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pky_Im1jOXWc",
        "outputId": "75437d4d-82ab-4c73-dcaf-ea25d84824ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7 CSV files in the folder:\n",
            "Name: Q2_2023_PxQ_NA.csv, ID: 11SBewICln91AVwufXPJcGAyBUlyTgYwJ\n",
            "Name: Q3_2023_PxQ_NA.csv, ID: 11feNZ5fEJEszROpHOIHEJE9DKCTccF40\n",
            "Name: Q3_2024_PxQ_NA.csv, ID: 11hKKdAT0H7tfOMZOSOh86RBeY5Ew27Zg\n",
            "Name: Q1_2024_PxQ_NA.csv, ID: 11NcjxD3USeFNMmUfvORYqyvaPN7xa3r7\n",
            "Name: Q4_2023_PxQ_NA.csv, ID: 11lHr1pUUkkTVM8Nh57R6BS4OVLJ9rJ3q\n",
            "Name: Q2_2024_PxQ_NA.csv, ID: 11T36kxa7i_csfJThfPkinxh3xzvRz8DU\n",
            "Name: Q4_2024_PxQ_NA.csv, ID: 11l_SNE1MZjyn7skhl4F9oCjs34taTbjH\n",
            "Combined DataFrame:\n",
            "     Country Cust ID Transaction Date     Amount  Activity  Cube Price Cycle  \\\n",
            "0  Argentina   AK001       2023-06-01   21274.65     579.0    0.045452   MAR   \n",
            "1  Argentina   AK001       2023-05-01   21274.65     579.0    0.045452   MAR   \n",
            "2  Argentina   AK001       2023-04-01   42549.30    1158.0    0.045452   MAR   \n",
            "3  Argentina   AK002       2023-06-01  204600.74    7355.0    0.034411   MAR   \n",
            "4  Argentina   AK002       2023-05-01  113418.39    7357.0    0.034411   MAR   \n",
            "\n",
            "             Max_Month  Current Year  Next Year  Prior Year        FX  \\\n",
            "0  2024-12-01 00:00:00        2024.0     2025.0      2023.0  0.001237   \n",
            "1  2024-12-01 00:00:00        2024.0     2025.0      2023.0  0.001237   \n",
            "2  2024-12-01 00:00:00        2024.0     2025.0      2023.0  0.001237   \n",
            "3  2024-12-01 00:00:00        2024.0     2025.0      2023.0  0.001237   \n",
            "4  2024-12-01 00:00:00        2024.0     2025.0      2023.0  0.001237   \n",
            "\n",
            "  Low Rate Adjust        Q  \n",
            "0          Normal  Q2_2023  \n",
            "1          Normal  Q2_2023  \n",
            "2          Normal  Q2_2023  \n",
            "3          Normal  Q2_2023  \n",
            "4          Normal  Q2_2023  \n",
            "Data successfully loaded into irm-fin-acct-dp-prod.Etl_alt_BQ.PXQ_NA_Clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Google Colab\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up Google Drive API\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Specify the folder ID\n",
        "folder_id = \"1wFqzM5wYLb2LFEKsQnVhH82LJ1FgC0Ve\"  # Your folder ID\n",
        "\n",
        "# Search for CSV files in the folder\n",
        "query = f\"'{folder_id}' in parents and mimeType='text/csv'\"\n",
        "response = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
        "files = response.get('files', [])\n",
        "\n",
        "if not files:\n",
        "    print(\"No CSV files found in the folder.\")\n",
        "else:\n",
        "    print(f\"Found {len(files)} CSV files in the folder:\")\n",
        "    for file in files:\n",
        "        print(f\"Name: {file['name']}, ID: {file['id']}\")\n",
        "\n",
        "# Combine all CSV files into a single DataFrame\n",
        "combined_data = pd.DataFrame()\n",
        "for file in files:\n",
        "    file_name = file['name']\n",
        "    file_id = file['id']\n",
        "\n",
        "    # Download the file content\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    file_path = f\"/content/{file_name}\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(request.execute())\n",
        "\n",
        "    # Load the CSV into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "\n",
        "# Preview the combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Set up BigQuery client\n",
        "project_id = \"irm-fin-acct-dp-prod\"\n",
        "dataset_id = \"Etl_alt_BQ\"\n",
        "table_id = \"NA_price_movements_Clean\"\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Auto-detect schema and load the combined DataFrame into BigQuery\n",
        "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "# Load the combined data to BigQuery\n",
        "job = client.load_table_from_dataframe(combined_data, f\"{project_id}.{dataset_id}.{table_id}\", job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f\"Data successfully loaded into {project_id}.{dataset_id}.{table_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7YAizePQObY",
        "outputId": "b4d71395-7dc5-48e0-f38f-9ba36ea00375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8 CSV files in the folder:\n",
            "Name: Q1_2024_NA_Price_Movements.csv, ID: 11NUj4Lqtk2JUvtdQzANqdGL-XHlemUw3\n",
            "Name: Q4_2024_NA_Price_Movements.csv, ID: 11KQt_07hdtycUbvFlGQ0mJGgpAzEu6PS\n",
            "Name: Q3_2024_NA_Price_Movements.csv, ID: 11EaN99WiVJoMqsZZXKdNKC5deij5zQ9Q\n",
            "Name: Q1_2023_NA_Price_Movements.csv, ID: 10qbywdElc-kzi8D2jwjnQeY7ySZblmkz\n",
            "Name: Q3_2023_NA_Price_Movements.csv, ID: 113wCH-8dunc0al6VYhnPABPS5WpY1NMY\n",
            "Name: Q2_2023_NA_Price_Movements.csv, ID: 116UiLfKhCelW8TybSzHsJFiUWX82mksv\n",
            "Name: Q4_2023_NA_Price_Movements.csv, ID: 11FPfYQwYv-Far7mK6x1HjqkgRYah21In\n",
            "Name: Q2_2024_NA_Price_Movements.csv, ID: 10j5PR7RTuOoZB8ld5IWaIlhdZOnTSb6Z\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-e47bd67f39bf>:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
            "<ipython-input-2-e47bd67f39bf>:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
            "<ipython-input-2-e47bd67f39bf>:41: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_data = pd.concat([combined_data, df], ignore_index=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame:\n",
            "          Country Cust ID Ranking Transaction Date Cycle  Total Amount  \\\n",
            "0       Indonesia   21150  Rank 1       2024-02-01   MAA     609040.00   \n",
            "1   United States   M0167  Rank 1       2024-02-01   MAA       1256.57   \n",
            "2          Canada   E6902  Rank 1       2024-03-01   MAA       1451.30   \n",
            "3   United States   SKA9E  Rank 1       2024-03-01   MAA       1166.74   \n",
            "4  Czech Republic   CZH85  Rank 1       2024-01-01   MAA       3578.69   \n",
            "\n",
            "   Unit Quantity    Unit Rate  Price Movement  Price Revenue Impact  \\\n",
            "0         135.00  4511.407407      762.407407             102925.00   \n",
            "1         637.20     1.972018        0.000000                  0.00   \n",
            "2        1358.89     1.068004        0.000000                  0.00   \n",
            "3        4704.60     0.248000        0.000000                  0.00   \n",
            "4         654.00     5.472003        0.591009                386.52   \n",
            "\n",
            "             Name        MOM        Q  \n",
            "0  Price Increase  290375.00  Q1_2024  \n",
            "1         Nothing       0.00  Q1_2024  \n",
            "2         Nothing       0.00  Q1_2024  \n",
            "3         Nothing       0.00  Q1_2024  \n",
            "4  Price Increase     386.52  Q1_2024  \n",
            "Data successfully loaded into irm-fin-acct-dp-prod.Etl_alt_BQ.NA_price_movements_Clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Google Colab\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up Google Drive API\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Specify the folder ID\n",
        "folder_id = \"12fDqdHNYq3PokSZrRNJSVggAwOzfqnjn\"  # Your folder ID\n",
        "\n",
        "# Search for CSV files in the folder\n",
        "query = f\"'{folder_id}' in parents and mimeType='text/csv'\"\n",
        "response = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
        "files = response.get('files', [])\n",
        "\n",
        "if not files:\n",
        "    print(\"No CSV files found in the folder.\")\n",
        "else:\n",
        "    print(f\"Found {len(files)} CSV files in the folder:\")\n",
        "    for file in files:\n",
        "        print(f\"Name: {file['name']}, ID: {file['id']}\")\n",
        "\n",
        "# Combine all CSV files into a single DataFrame\n",
        "combined_data = pd.DataFrame()\n",
        "for file in files:\n",
        "    file_name = file['name']\n",
        "    file_id = file['id']\n",
        "\n",
        "    # Download the file content\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    file_path = f\"/content/{file_name}\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(request.execute())\n",
        "\n",
        "    # Load the CSV into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "\n",
        "# Preview the combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Set up BigQuery client\n",
        "project_id = \"irm-fin-acct-dp-prod\"\n",
        "dataset_id = \"Etl_alt_BQ\"\n",
        "table_id = \"NA_Debits_Credits_Clean\"\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Auto-detect schema and load the combined DataFrame into BigQuery\n",
        "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "# Load the combined data to BigQuery\n",
        "job = client.load_table_from_dataframe(combined_data, f\"{project_id}.{dataset_id}.{table_id}\", job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f\"Data successfully loaded into {project_id}.{dataset_id}.{table_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwIA0_4k04RX",
        "outputId": "5131365c-9077-40a7-a599-11832aeeff44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 CSV files in the folder:\n",
            "Name: NA Debits & Credits.csv, ID: 12gKwFbLAM5yW1PGJKE8Zvoeo7NZ63eq2\n",
            "Combined DataFrame:\n",
            "     Cycle Transaction Date        Country   Cust ID      Oracle ID  \\\n",
            "0   Debits       2023-09-25         Canada     80519   07731.080519   \n",
            "1  Credits       2023-05-09       Thailand  RE336818       30000275   \n",
            "2  Credits       2023-01-31  United States     A3708   01322.0A3708   \n",
            "3  Credits       2023-03-08  United States     IS475   07330.0IS475   \n",
            "4  Credits       2023-02-01        Ireland     IC469  8251100.IC469   \n",
            "\n",
            "                         Customer Name    Amount  \n",
            "0   WORKPLACE SAFETY & INSURANCE BOARD  10834.80  \n",
            "1                    TILLEKE & GIBBINS -49048.80  \n",
            "2   MEDICAL INFORMATION MANAGEMENT SOL     -2.92  \n",
            "3  MMG-MCLAREN MED GRP BAY INTERNAL ME   -208.28  \n",
            "4  SOUTH INFIRMARY VICTORIA-ECG/HOLTER    -20.35  \n",
            "Data successfully loaded into irm-fin-acct-dp-prod.Etl_alt_BQ.NA_Debits_Credits_Clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Google Colab\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up Google Drive API\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Specify the folder ID\n",
        "folder_id = \"12dSomeFN1jHJnMUsyKdGnyAcGO78t68v\"  # Your folder ID\n",
        "\n",
        "# Search for CSV files in the folder\n",
        "query = f\"'{folder_id}' in parents and mimeType='text/csv'\"\n",
        "response = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
        "files = response.get('files', [])\n",
        "\n",
        "if not files:\n",
        "    print(\"No CSV files found in the folder.\")\n",
        "else:\n",
        "    print(f\"Found {len(files)} CSV files in the folder:\")\n",
        "    for file in files:\n",
        "        print(f\"Name: {file['name']}, ID: {file['id']}\")\n",
        "\n",
        "# Combine all CSV files into a single DataFrame\n",
        "combined_data = pd.DataFrame()\n",
        "for file in files:\n",
        "    file_name = file['name']\n",
        "    file_id = file['id']\n",
        "\n",
        "    # Download the file content\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    file_path = f\"/content/{file_name}\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(request.execute())\n",
        "\n",
        "    # Load the CSV into a DataFrame\n",
        "    df = pd.read_csv(file_path)\n",
        "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "\n",
        "# Preview the combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(combined_data.head())\n",
        "\n",
        "# Set up BigQuery client\n",
        "project_id = \"irm-fin-acct-dp-prod\"\n",
        "dataset_id = \"Etl_alt_BQ\"\n",
        "table_id = \"NA_Customer_Names_Clean\"\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Auto-detect schema and load the combined DataFrame into BigQuery\n",
        "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "# Load the combined data to BigQuery\n",
        "job = client.load_table_from_dataframe(combined_data, f\"{project_id}.{dataset_id}.{table_id}\", job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f\"Data successfully loaded into {project_id}.{dataset_id}.{table_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU1mSYB22rwB",
        "outputId": "3ecbd806-77c1-4064-e0e9-d3ce8fbac662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1 CSV files in the folder:\n",
            "Name: NA Customer Names.csv, ID: 12dd79_Gx9dgB4gzcmqRsL5Dngql_cZLZ\n",
            "Combined DataFrame:\n",
            "         Country   Cust ID         Oracle ID  \\\n",
            "0      Australia  ON400467  9082113.17067066   \n",
            "1  United States     J7317      07213.0J7317   \n",
            "2  United States     CF869      01222.0CF869   \n",
            "3         France     FY524   10683418.SFCORE   \n",
            "4          Chile     CSF57         656775009   \n",
            "\n",
            "                         Customer Name  \n",
            "0                     TRANSOCEAN GROUP  \n",
            "1  NEW JERSEY STATE NURSES ASSOCIATION  \n",
            "2     ACC CAPITAL HOLDINGS CORPORATION  \n",
            "3                  BIOTRIAL RENNES SAS  \n",
            "4  COOPERATIVA AHORRO Y CRED FINANCOOP  \n",
            "Data successfully loaded into irm-fin-acct-dp-prod.Etl_alt_BQ.NA_Customer_Names_Clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "\n",
        "# Authenticate with Google Colab\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up Google Drive API\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Specify the folder ID\n",
        "folder_id = \"1BuuL3rW-lpJZTAiMdP4oPqw4JQGaqBx3\"  # Your folder ID\n",
        "\n",
        "# Search for CSV files in the folder\n",
        "query = f\"'{folder_id}' in parents and mimeType='text/csv'\"\n",
        "response = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
        "files = response.get('files', [])\n",
        "\n",
        "if not files:\n",
        "    print(\"No CSV files found in the folder.\")\n",
        "else:\n",
        "    print(f\"Found {len(files)} CSV files in the folder:\")\n",
        "    for i, file in enumerate(files):\n",
        "        print(f\"{i + 1}. Name: {file['name']}, ID: {file['id']}\")\n",
        "\n",
        "# Combine all CSV files into a single DataFrame\n",
        "combined_data = pd.DataFrame()\n",
        "for file in files:\n",
        "    file_name = file['name']\n",
        "    file_id = file['id']\n",
        "\n",
        "    # Download the file content\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    file_path = f\"/content/{file_name}\"\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        f.write(request.execute())\n",
        "\n",
        "    # Load the CSV into a DataFrame\n",
        "    df = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "    # Sanitize column names\n",
        "    df.columns = [col.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\") for col in df.columns]\n",
        "\n",
        "    # Append to combined DataFrame\n",
        "    combined_data = pd.concat([combined_data, df], ignore_index=True)\n",
        "\n",
        "# Ensure column consistency by filling missing columns\n",
        "expected_columns = combined_data.columns.tolist()  # Get the union of all columns\n",
        "combined_data = combined_data.reindex(columns=expected_columns, fill_value=\"Unknown\")\n",
        "\n",
        "# Convert numeric columns to the correct type\n",
        "for column in combined_data.columns:\n",
        "    if combined_data[column].dtype == 'object':  # String columns\n",
        "        combined_data[column] = combined_data[column].fillna(\"Unknown\")\n",
        "    elif pd.api.types.is_numeric_dtype(combined_data[column]):  # Numeric columns\n",
        "        combined_data[column] = pd.to_numeric(combined_data[column], errors='coerce').fillna(0)\n",
        "\n",
        "# Preview combined DataFrame\n",
        "print(\"Combined DataFrame:\")\n",
        "print(combined_data.head())\n",
        "print(f\"Total rows in combined DataFrame: {len(combined_data)}\")\n",
        "\n",
        "# Set up BigQuery client\n",
        "project_id = \"irm-fin-acct-dp-prod\"\n",
        "dataset_id = \"Etl_alt_BQ\"\n",
        "table_id = \"NA_Fixed_Billing_Clean\"\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Auto-detect schema and load the combined DataFrame into BigQuery\n",
        "job_config = bigquery.LoadJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "# Load the combined data to BigQuery\n",
        "job = client.load_table_from_dataframe(combined_data, f\"{project_id}.{dataset_id}.{table_id}\", job_config=job_config)\n",
        "job.result()  # Wait for the job to complete\n",
        "\n",
        "print(f\"Data successfully loaded into {project_id}.{dataset_id}.{table_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siwD1xRu3iHU",
        "outputId": "93ef54ee-e015-474c-be21-2045920a14d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 CSV files in the folder:\n",
            "1. Name: NA_Fixed_billing_NA.csv, ID: 12V-AhcuHeNVpY3y9hl1KwKR9HsKfSs1P\n",
            "2. Name: NA_Fixed_billing_EMEA.csv, ID: 12BESccM2eYLZ4YIxYxsZGmnPofNYUKcc\n",
            "3. Name: NA_Fixed_billing_APAC.csv, ID: 12VPUDkUATluXtlqXJ4a3Yudk6VMxCV6I\n",
            "4. Name: NA_Fixed_billing_LATAM.csv, ID: 12OwvtU8gIJKMz3pR9tD5X5A1cur2swaR\n",
            "5. Name: NA_Fixed_billing_INDIA.csv, ID: 12FIaxS28SCiXTEJB2j6n6RLeKFsF1qlR\n",
            "Combined DataFrame:\n",
            "  billing_system invoicing_country_code calendar_date        Date    INV_#  \\\n",
            "0            SKP                    USA    2024-10-31  2024-10-31  JWRJ166   \n",
            "1            SKP                    USA    2024-08-31  2024-08-31  JSZK971   \n",
            "2            SKP                    USA    2024-03-31  2024-03-31  JJDB650   \n",
            "3            SKP                    USA    2024-06-30  2024-06-30  JNZT886   \n",
            "4            SKP                    USA    2024-11-30  2024-11-30  JYKW202   \n",
            "\n",
            "  Year_&_Month        Country Cluster   Region Cust_ID  ...  Cycle  \\\n",
            "0      2024-10  United States    East  Unknown   NG171  ...  Fixed   \n",
            "1      2024-08  United States    East  Unknown   NG171  ...  Fixed   \n",
            "2      2024-03  United States    East  Unknown   NG171  ...  Fixed   \n",
            "3      2024-06  United States    East  Unknown   NG171  ...  Fixed   \n",
            "4      2024-11  United States    East  Unknown   NG171  ...  Fixed   \n",
            "\n",
            "   Right_Country Standard   QAA    SA   AAA Storage_Start_Date    Year  \\\n",
            "0  United States      5.0  15.0  30.0  60.0         2024-11-01  2024.0   \n",
            "1  United States      5.0  15.0  30.0  60.0         2024-09-01  2024.0   \n",
            "2  United States      5.0  15.0  30.0  60.0         2024-04-01  2024.0   \n",
            "3  United States      5.0  15.0  30.0  60.0         2024-07-01  2024.0   \n",
            "4  United States      5.0  15.0  30.0  60.0         2024-12-01  2024.0   \n",
            "\n",
            "  Storage_End_Date  Number_of_Storage_Months  \n",
            "0       2024-11-30                       0.0  \n",
            "1       2024-09-30                       0.0  \n",
            "2       2024-04-30                       0.0  \n",
            "3       2024-07-31                       0.0  \n",
            "4       2024-12-31                       0.0  \n",
            "\n",
            "[5 rows x 49 columns]\n",
            "Total rows in combined DataFrame: 1660549\n",
            "Data successfully loaded into irm-fin-acct-dp-prod.Etl_alt_BQ.NA_Fixed_Billing_Clean\n"
          ]
        }
      ]
    }
  ]
}